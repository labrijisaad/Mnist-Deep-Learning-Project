{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "432f624d-dc99-4009-9391-d1c166997f2b",
   "metadata": {
    "id": "432f624d-dc99-4009-9391-d1c166997f2b"
   },
   "source": [
    "## Import the needed `Libraries`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a31f840-b0e3-4e17-bed6-c8aa3fc2af62",
   "metadata": {
    "id": "2a31f840-b0e3-4e17-bed6-c8aa3fc2af62"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras import layers,models\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0247b4a2-ca9b-4e01-b2b1-b5b3063e498b",
   "metadata": {
    "id": "0247b4a2-ca9b-4e01-b2b1-b5b3063e498b"
   },
   "source": [
    "## Load the `MNIST degit` data from `keras`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e2cf64-4150-4104-ba64-42b4909ba38d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a5e2cf64-4150-4104-ba64-42b4909ba38d",
    "outputId": "ffb9c917-185e-4d79-fa0e-1e1fe1b25155"
   },
   "outputs": [],
   "source": [
    "def load_mnist():\n",
    "    \"\"\"\n",
    "    Load the MNIST dataset.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of NumPy arrays : (x_train_full, y_train_full, x_test, y_test)\n",
    "    \"\"\"\n",
    "    (x_train_full, y_train_full), (x_test, y_test) = mnist.load_data()\n",
    "    return x_train_full, y_train_full, x_test, y_test\n",
    "\n",
    "# Separate the train/test data\n",
    "x_train_full, y_train_full, x_test, y_test = load_mnist()\n",
    "\n",
    "# Print the length of the loaded data\n",
    "print(f\"Number of training samples: {len(x_train_full)}\")\n",
    "print(f\"Number of testing samples: {len(x_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438c3a59-4c74-4119-be97-044528e97d4d",
   "metadata": {
    "id": "438c3a59-4c74-4119-be97-044528e97d4d"
   },
   "source": [
    "We're using a dataset of **60,000 images** to train a semi-supervised model. Out of these, **100 images are labeled**, and the remaining **59,900 are unlabeled**. After training, we'll assess the model's performance on **10,000 test images**.\n",
    "\n",
    "> **Note:** The 100 labeled images might not be sufficient (we'll explain later). To address this, we'll employ `data augmentation` on the labeled set. Afterward, we'll retrain the model and compare performances. Simultaneously, we'll experiment with **different models** to identify which one learns the most from our data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7549bd9-ca28-4507-8b8b-cf8c267e27c6",
   "metadata": {
    "id": "d7549bd9-ca28-4507-8b8b-cf8c267e27c6"
   },
   "source": [
    "## Selecting `Train/Test Data` with Respect to Data Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vBxIOBeHo3mS",
   "metadata": {
    "id": "vBxIOBeHo3mS"
   },
   "outputs": [],
   "source": [
    "def get_class_indices(y, class_label, count):\n",
    "    \"\"\"\n",
    "    Get random indices for a specific class.\n",
    "\n",
    "    Args:\n",
    "        y (np.ndarray): Labels.\n",
    "        class_label (int): Class label.\n",
    "        count (int): Number of indices to select.\n",
    "\n",
    "    Returns:\n",
    "        NumPy array of indices.\n",
    "    \"\"\"\n",
    "    class_indices = np.where(y == class_label)[0]\n",
    "    return np.random.choice(class_indices, size=count, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31137664-9813-4c5e-9bfa-769e7c47e58e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "31137664-9813-4c5e-9bfa-769e7c47e58e",
    "outputId": "dbc796dc-5c13-4725-e217-b67f11430048"
   },
   "outputs": [],
   "source": [
    "def select_labeled_and_unlabeled_data_with_validation(x_train_full, y_train_full, labeled_count_per_class, validation_count, test_unlabeled_count):\n",
    "    \"\"\"\n",
    "    Select a small subset for labeled data, use the rest as unlabeled,\n",
    "    and select a fixed number of samples for validation and unlabeled for testing.\n",
    "\n",
    "    Args:\n",
    "        x_train_full (np.ndarray): Full set of input images.\n",
    "        y_train_full (np.ndarray): Full set of labels.\n",
    "        labeled_count_per_class (int): Number of labeled examples per class.\n",
    "        validation_count (int): Number of samples for the validation set.\n",
    "        test_unlabeled_count (int): Number of samples for the unlabeled test set.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of NumPy arrays: (x_labeled, y_labeled, x_unlabeled, x_validation, y_validation)\n",
    "    \"\"\"\n",
    "    # Get unique classes in the training set\n",
    "    classes = np.unique(y_train_full)\n",
    "\n",
    "    # Select a small subset for labeled data\n",
    "    labeled_indices = np.concatenate([get_class_indices(y_train_full, label, labeled_count_per_class) for label in classes])\n",
    "    x_labeled = x_train_full[labeled_indices]\n",
    "    y_labeled = y_train_full[labeled_indices]\n",
    "\n",
    "    # Use the rest of the data as unlabeled\n",
    "    x_unlabeled = np.delete(x_train_full, labeled_indices, axis=0)\n",
    "    y_unlabeled = np.delete(y_train_full, labeled_indices, axis=0)\n",
    "\n",
    "    # Use StratifiedShuffleSplit to split the remaining data into training and validation sets\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=validation_count, random_state=42)\n",
    "\n",
    "    for train_index, validation_index in sss.split(x_unlabeled, y_unlabeled):\n",
    "        x_validation = x_unlabeled[validation_index]\n",
    "        y_validation = y_unlabeled[validation_index]\n",
    "        x_unlabeled = x_unlabeled[train_index]\n",
    "        y_unlabeled = y_unlabeled[train_index]\n",
    "\n",
    "    # Take the remaining samples as unlabeled for testing\n",
    "    x_unlabeled_test = x_unlabeled[:test_unlabeled_count]\n",
    "    y_unlabeled_test = y_unlabeled[:test_unlabeled_count]\n",
    "\n",
    "    # Shuffle x_labeled, x_validation, and x_unlabeled in the same way\n",
    "    shuffle_indices = np.random.permutation(len(x_labeled))\n",
    "    x_labeled = x_labeled[shuffle_indices]\n",
    "    y_labeled = y_labeled[shuffle_indices]\n",
    "\n",
    "    shuffle_indices = np.random.permutation(len(x_validation))\n",
    "    x_validation = x_validation[shuffle_indices]\n",
    "    y_validation = y_validation[shuffle_indices]\n",
    "\n",
    "    shuffle_indices = np.random.permutation(len(x_unlabeled))\n",
    "    x_unlabeled = x_unlabeled[shuffle_indices]\n",
    "    y_unlabeled_test = y_unlabeled_test[shuffle_indices]\n",
    "\n",
    "    return x_labeled, y_labeled, x_unlabeled, x_validation, y_validation\n",
    "\n",
    "# Example of using the function\n",
    "x_labeled, y_labeled, x_unlabeled, x_validation, y_validation = select_labeled_and_unlabeled_data_with_validation(\n",
    "    x_train_full, y_train_full, labeled_count_per_class=10, validation_count=100, test_unlabeled_count=59800\n",
    ")\n",
    "\n",
    "# Print information about labeled, unlabeled, and validation sets\n",
    "print(f\"Number of labeled samples in the train set: {len(x_labeled)}\")\n",
    "print(f\"Number of unique classes in labeled data: {len(np.unique(y_labeled))}\")\n",
    "for class_label in np.unique(y_labeled):\n",
    "    class_count = np.sum(y_labeled == class_label)\n",
    "    print(f\"-> Number of samples for class {class_label}: {class_count}\")\n",
    "\n",
    "print(f\"Number of unlabeled samples in the test set: {len(x_unlabeled)}\")\n",
    "\n",
    "print(f\"\\nNumber of samples in the validation set: {len(x_validation)}\")\n",
    "print(f\"Number of unique classes in the validation set: {len(np.unique(y_validation))}\")\n",
    "for class_label in np.unique(y_validation):\n",
    "    class_count = np.sum(y_validation == class_label)\n",
    "    print(f\"-> Number of samples for class {class_label}: {class_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f48cc45-8228-4c88-b825-219c093f6137",
   "metadata": {
    "id": "1f48cc45-8228-4c88-b825-219c093f6137"
   },
   "source": [
    "> **Note:** We kept the `data balanced` in the **target column** by randomly picking 100 images, 10 from each class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a456c673-0f70-499d-b0c9-4281cee21036",
   "metadata": {
    "id": "a456c673-0f70-499d-b0c9-4281cee21036"
   },
   "source": [
    "## `Exploring the Data`: Visualizing 10 Random Samples from the Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863f9d2f-2853-4230-a413-6e9941a12178",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "863f9d2f-2853-4230-a413-6e9941a12178",
    "outputId": "d646c22a-28e7-45ff-b353-9c6fc05270e6"
   },
   "outputs": [],
   "source": [
    "def display_random_images(x_data, y_data, num_images=10):\n",
    "    # Generate random indices\n",
    "    random_indices = np.random.choice(len(x_data), num_images, replace=False)\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axs = plt.subplots(1, num_images, figsize=(18, 2))\n",
    "\n",
    "    for i, index in enumerate(random_indices):\n",
    "        # Print the label\n",
    "        print(f\"Image {i + 1}, Label:\", y_data[index])\n",
    "\n",
    "        # Reshape the image and display it\n",
    "        img = x_data[index].reshape(28, 28)\n",
    "        axs[i].imshow(img, cmap='gray')\n",
    "        axs[i].axis('off')  # Turn off axis for cleaner display\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "# Assuming y_labeled and x_labeled are defined somewhere before calling the function\n",
    "display_random_images(x_validation, y_validation, num_images=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eeab77a-1cab-4425-9663-173f2206941a",
   "metadata": {
    "id": "6eeab77a-1cab-4425-9663-173f2206941a"
   },
   "source": [
    "Here, we **displayed 10 randomly** selected images from the training set with the aim of **visually understanding the nature of the data**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b8050c-aefe-4796-98e6-8b87fafdeeb9",
   "metadata": {
    "id": "b5b8050c-aefe-4796-98e6-8b87fafdeeb9"
   },
   "source": [
    "# `Normalize` the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f8eb97-ad8c-4ef2-ae9c-9c5522dd4d8f",
   "metadata": {
    "id": "c5f8eb97-ad8c-4ef2-ae9c-9c5522dd4d8f"
   },
   "outputs": [],
   "source": [
    "def normalize_data_with_validation(x_labeled,\n",
    "                                   x_unlabeled,\n",
    "                                   x_validation,\n",
    "                                   x_test):\n",
    "    \"\"\"\n",
    "    Normalize pixel values of input images to the range [0, 1].\n",
    "\n",
    "    Args:\n",
    "        x_labeled (np.ndarray): Labeled input images.\n",
    "        x_unlabeled (np.ndarray): Unlabeled input images.\n",
    "        x_validation (np.ndarray): Validation input images.\n",
    "        x_test (np.ndarray): Test input images.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of normalized NumPy arrays: (x_labeled, x_unlabeled, x_validation, x_test)\n",
    "    \"\"\"\n",
    "    x_labeled = x_labeled / 255.0\n",
    "    x_unlabeled = x_unlabeled / 255.0\n",
    "    x_validation = x_validation / 255.0\n",
    "    x_test = x_test / 255.0\n",
    "\n",
    "    return x_labeled, x_unlabeled, x_validation, x_test\n",
    "\n",
    "# Normalize the pixel values of the images to the range [0, 1], including validation set\n",
    "x_labeled, x_unlabeled, x_validation, x_test = normalize_data_with_validation(\n",
    "    x_labeled, x_unlabeled, x_validation, x_test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf1c35d-ca6f-413d-b511-f42965dccb46",
   "metadata": {
    "id": "7bf1c35d-ca6f-413d-b511-f42965dccb46"
   },
   "source": [
    "This code **normalizes** MNIST digits data by scaling pixel values to the range [0, 1], `aiding faster model training and ensuring consistency`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fa332c-c95d-4cbc-87b5-7aa4b5507245",
   "metadata": {
    "id": "58fa332c-c95d-4cbc-87b5-7aa4b5507245"
   },
   "source": [
    "-----\n",
    "# `Semi-Supervised` self-training CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2d1f99-1a98-4c85-9bd7-73da7f4e73f8",
   "metadata": {
    "id": "1e2d1f99-1a98-4c85-9bd7-73da7f4e73f8"
   },
   "source": [
    "### Defining the `Models` and Setting Up the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3280574f-6918-4494-a8cf-38fc61e684ad",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3280574f-6918-4494-a8cf-38fc61e684ad",
    "outputId": "930009f8-7d05-470a-fa42-b9af25c9f5de"
   },
   "outputs": [],
   "source": [
    "model_CNN = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),  # Convolutional layer with 32 filters\n",
    "    layers.MaxPooling2D((2, 2)),                                            # Max pooling layer\n",
    "    layers.Flatten(),                                                       # Flatten for fully connected layers\n",
    "    layers.Dense(128, activation='relu'),                                   # Fully connected layer with 128 units and ReLU activation\n",
    "    layers.Dense(10, activation='softmax')                                  # Output layer with 10 units and softmax activation\n",
    "])\n",
    "\n",
    "# Display the model summary\n",
    "model_CNN.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526fc1d6-76ac-4fa5-b704-b36a2ff78506",
   "metadata": {
    "id": "526fc1d6-76ac-4fa5-b704-b36a2ff78506"
   },
   "source": [
    "### `Training` and `Testing` the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072b6214-5b5e-4f4a-a40c-6811eb5896e8",
   "metadata": {
    "id": "072b6214-5b5e-4f4a-a40c-6811eb5896e8"
   },
   "outputs": [],
   "source": [
    "def pseudo_labeling(model, x_unlabeled, confidence_threshold=0.8):\n",
    "    \"\"\"\n",
    "    Perform pseudo-labeling on unlabeled data using the given model.\n",
    "\n",
    "    Parameters:\n",
    "        model: Trained model for pseudo-labeling.\n",
    "        x_unlabeled: Unlabeled input images.\n",
    "        confidence_threshold: Confidence threshold for pseudo-labeling.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of numpy arrays: (pseudo_labeled_images, pseudo_labels, x_unlabeled_unused)\n",
    "    \"\"\"\n",
    "    # Predict pseudo-labels for the unlabeled data using the provided model\n",
    "    pseudo_labels = model.predict(x_unlabeled)\n",
    "\n",
    "    # Create a mask to identify instances with confidence above the threshold\n",
    "    confident_mask = np.max(pseudo_labels, axis=1) > confidence_threshold\n",
    "\n",
    "    # Check if there are instances with confidence above the threshold\n",
    "    num_confident_instances = np.sum(confident_mask)\n",
    "    total_instances = len(x_unlabeled)\n",
    "    print(f\"\\n\\nNumber of instances with confidence > {confidence_threshold}: {num_confident_instances} out of {total_instances}\\n\\n\")\n",
    "\n",
    "    # Check if there are instances with confidence above the threshold\n",
    "    if num_confident_instances > 0:\n",
    "        # Extract pseudo-labeled images and their corresponding labels\n",
    "        pseudo_labeled_images = x_unlabeled[confident_mask]\n",
    "        pseudo_labels = np.argmax(pseudo_labels[confident_mask], axis=1)\n",
    "\n",
    "        # Extract unused unlabeled images\n",
    "        x_unlabeled_unused = x_unlabeled[~confident_mask]\n",
    "\n",
    "        return pseudo_labeled_images, pseudo_labels, x_unlabeled_unused\n",
    "    else:\n",
    "        # Return None if no instances meet the confidence threshold\n",
    "        return None, None, x_unlabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-YitQOkoZyCp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-YitQOkoZyCp",
    "outputId": "e01bbc53-5262-4835-a8a9-9a79d8fafa7f"
   },
   "outputs": [],
   "source": [
    "# Selecting the model and hyperparameters\n",
    "classifier1, classifier2 = model_CNN, model_CNN\n",
    "epochs_classifier1, batch_size_classifier1= 150, 32\n",
    "epochs_classifier2, batch_size_classifier2= 100, 2000\n",
    "\n",
    "# Compile the classifier1 and classifier2\n",
    "classifier1.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "classifier2.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Use EarlyStopping to avoid wasting time\n",
    "early_stop = EarlyStopping(monitor='loss', patience=3, verbose=1)\n",
    "\n",
    "# Print the number of elements used to train the first classifier\n",
    "print(f\">>>> Number of elements used to train the first classifier: {len(x_labeled)}\")\n",
    "\n",
    "# Train the model with the original labeled data and validation set, and save the history\n",
    "history_classifier1 = classifier1.fit(\n",
    "    x_labeled,\n",
    "    y_labeled,\n",
    "    epochs=epochs_classifier1,\n",
    "    batch_size=batch_size_classifier1,\n",
    "    validation_data=(x_validation, y_validation),\n",
    "    callbacks=[early_stop]\n",
    ")\n",
    "\n",
    "# Pseudo-labeling\n",
    "pseudo_labeled_images, pseudo_labels, x_unlabeled_unused = pseudo_labeling(classifier1, \n",
    "                                                                           x_unlabeled,\n",
    "                                                                           confidence_threshold=0.95)\n",
    "\n",
    "if pseudo_labeled_images is not None:\n",
    "    # Update labeled data with pseudo-labeled data\n",
    "    x_labeled_and_pseudo_labeled_images = np.concatenate([x_labeled, pseudo_labeled_images])\n",
    "    y_labeled_and_pseudo_labeled_images = np.concatenate([y_labeled, pseudo_labels])\n",
    "\n",
    "    # Print the number of elements used to train the first classifier and pseudo-labeled data\n",
    "    print(f\">>>> Number of elements used to train the second classifier : {len(x_labeled_and_pseudo_labeled_images)+len(x_unlabeled_unused)}\")\n",
    "\n",
    "    # Train the classifier2 on the full dataset (original + pseudo-labeled) with validation set, and save the history\n",
    "    history_classifier2 = classifier2.fit(\n",
    "        np.concatenate([x_labeled_and_pseudo_labeled_images, x_unlabeled_unused]),\n",
    "        np.concatenate([y_labeled_and_pseudo_labeled_images, np.argmax(classifier2.predict(x_unlabeled_unused), axis=1)]),\n",
    "        epochs=epochs_classifier2,\n",
    "        batch_size=batch_size_classifier2,\n",
    "        validation_data=(x_validation, y_validation),\n",
    "        callbacks=[early_stop]\n",
    "    )\n",
    "\n",
    "# Evaluate the classifier2 Performance on Test Data\n",
    "test_loss, test_acc = classifier2.evaluate(x_test, y_test)\n",
    "print(f\"classifier2 - Test Loss: {test_loss}, Test Accuracy: {test_acc}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c5de8e-0551-4802-bd02-8d6ab12042a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_print_classification_metrics(y_true, y_pred):\n",
    "    # Create a confusion matrix\n",
    "    conf_mat = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_mat, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=np.arange(10), yticklabels=np.arange(10))\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # Classification report\n",
    "    class_report = classification_report(y_true, y_pred, target_names=[str(i) for i in range(10)])\n",
    "    print(\"Classification Report:\\n\", class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4902d1-9dbe-4119-91b4-8e7ac0952c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on the test set using classifier2\n",
    "y_pred = np.argmax(classifier2.predict(x_test), axis=1)\n",
    "\n",
    "# Use the function to plot and print metrics\n",
    "plot_and_print_classification_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a4d5e3-1bca-4332-a222-3f5768055e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training loss and validation accuracy for both classifiers with improved aesthetics\n",
    "def plot_metrics(history, title):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Plot training and validation loss\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss', color='blue', marker='o')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss', color='orange', marker='o')\n",
    "    plt.title(f'{title} Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "    # Plot training and validation accuracy\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy', color='green', marker='o')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy', color='red', marker='o')\n",
    "    plt.title(f'{title} Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "    plt.tight_layout(pad=3.0)\n",
    "    plt.show()\n",
    "\n",
    "# Plot the training loss and validation accuracy for each classifier\n",
    "plot_metrics(history_classifier1, 'Classifier 1 without Data Augmentation')\n",
    "plot_metrics(history_classifier2, 'Classifier 2 without Data Augmentation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc982171-11b6-44e6-abb7-5d79ae9d8386",
   "metadata": {
    "id": "cc982171-11b6-44e6-abb7-5d79ae9d8386"
   },
   "source": [
    "### `Testing` the Model on a Random MNIST Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c2a27a-cbda-459b-acf6-ceb623797510",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "56c2a27a-cbda-459b-acf6-ceb623797510",
    "outputId": "6574944e-ac3e-4e5a-daa4-0b38190d74e8"
   },
   "outputs": [],
   "source": [
    "def visualize_predictions(model, x_test, y_test, num_samples):\n",
    "    \"\"\"\n",
    "    Visualize predictions on a few test images.\n",
    "\n",
    "    Args:\n",
    "        model (keras.Model): Trained neural network model.\n",
    "        x_test (np.ndarray): Test input images.\n",
    "        y_test (np.ndarray): Test labels.\n",
    "        num_samples (int): Number of test samples to visualize. Default is 5.\n",
    "    \"\"\"\n",
    "    # Randomly select test samples\n",
    "    test_indices = np.random.choice(range(len(x_test)), size=num_samples, replace=False)\n",
    "\n",
    "    # Iterate over selected test samples\n",
    "    for index in test_indices:\n",
    "        test_image = x_test[index].reshape(1, 28, 28)  # Reshape for prediction\n",
    "        prediction = model.predict(test_image)\n",
    "\n",
    "        # Display the test image along with true and predicted labels\n",
    "        plt.figure()\n",
    "        plt.imshow(test_image.squeeze(), cmap='gray')\n",
    "        plt.title(f\"True Label: {y_test[index]}, Predicted Label: {np.argmax(prediction)}\")\n",
    "        plt.show()\n",
    "\n",
    "num_samples = 5\n",
    "visualize_predictions(classifier2, x_test, y_test, num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2841715-c350-4b3b-b578-e29530c2453c",
   "metadata": {
    "id": "d2841715-c350-4b3b-b578-e29530c2453c"
   },
   "source": [
    "# Data `Augmentation`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c94606-8fcd-4cd2-99d6-4aae72dfda9b",
   "metadata": {},
   "source": [
    "#### Data `Augmentation` function for an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86664119-de33-4aa5-9a4f-d312c1118e14",
   "metadata": {
    "id": "86664119-de33-4aa5-9a4f-d312c1118e14"
   },
   "outputs": [],
   "source": [
    "def augment_data(image, augment_factor=8):\n",
    "    \"\"\"\n",
    "    Apply diverse data augmentation to a given image.\n",
    "\n",
    "    Parameters:\n",
    "        image (numpy.ndarray): Input image to be augmented.\n",
    "        augment_factor (int): Number of augmented images to generate.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Array of augmented images.\n",
    "    \"\"\"\n",
    "    augmented_images = []\n",
    "    for _ in range(augment_factor):\n",
    "        # Random rotation (between -10 and 10 degrees)\n",
    "        angle = np.random.uniform(-10, 10)\n",
    "        # Random translation (between -3 and 3 pixels in both x and y directions)\n",
    "        tx = np.random.uniform(-3, 3)\n",
    "        ty = np.random.uniform(-3, 3)\n",
    "        # Random zoom (between 0.9 and 1.1)\n",
    "        zoom = np.random.uniform(0.9, 1.1)\n",
    "        # Random brightness adjustment\n",
    "        brightness_factor = np.random.uniform(0.7, 1.3)\n",
    "        # Random contrast adjustment\n",
    "        contrast_factor = np.random.uniform(0.7, 1.3)\n",
    "        # Apply transformations\n",
    "        transform_matrix = cv2.getRotationMatrix2D((14, 14), angle, zoom)\n",
    "        transform_matrix[:, 2] += (tx, ty)\n",
    "        augmented_image = cv2.warpAffine(image[0], transform_matrix, (28, 28), flags=cv2.INTER_LINEAR)\n",
    "        # Adjust brightness and contrast\n",
    "        augmented_image = cv2.convertScaleAbs(augmented_image, alpha=brightness_factor, beta=contrast_factor)\n",
    "        augmented_images.append(augmented_image.reshape(1, 28, 28, 1))\n",
    "    augmented_images = np.vstack(augmented_images)\n",
    "    return augmented_images\n",
    "\n",
    "# Utility function to test the Data Augmentation on sample images\n",
    "def visualize_images(images, title, labels):\n",
    "    \"\"\"\n",
    "    Display images with their corresponding labels.\n",
    "\n",
    "    Parameters:\n",
    "        images (List[numpy.ndarray]): List of images to be visualized.\n",
    "        title (str): Title for the visualization.\n",
    "        labels (numpy.ndarray): Array of labels corresponding to the images.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 3))\n",
    "    for i in range(len(images)):\n",
    "        plt.subplot(1, len(images), i + 1)\n",
    "        plt.imshow(images[i].reshape(28, 28), cmap='gray')\n",
    "        plt.title(f\"Label: {labels[i]}\")\n",
    "        plt.axis('off')\n",
    "    plt.suptitle(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75848c31-cd86-4011-b095-7f2c6781e5ca",
   "metadata": {
    "id": "75848c31-cd86-4011-b095-7f2c6781e5ca"
   },
   "source": [
    "## Testing `Data Augmentation` on a random MNIST image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad37bbf6-02c7-40d0-bd0a-9b0242fdeeaf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "id": "ad37bbf6-02c7-40d0-bd0a-9b0242fdeeaf",
    "outputId": "e8fa7263-23ab-4a74-80ac-538f90d5c753"
   },
   "outputs": [],
   "source": [
    "# Select one image for demonstration\n",
    "demo_index = np.random.choice(range(len(x_train_full)), size=1, replace=False)\n",
    "demo_image = x_train_full[demo_index]\n",
    "demo_label = y_train_full[demo_index]\n",
    "\n",
    "# Display original image\n",
    "visualize_images([demo_image], \"\", demo_label)\n",
    "\n",
    "# Data augmentation for demonstration\n",
    "augmented_images = augment_data(demo_image, augment_factor=10)\n",
    "\n",
    "# Display augmented images\n",
    "augmented_labels = np.full(len(augmented_images), demo_label[0])\n",
    "visualize_images(augmented_images, \"Augmented Images\", augmented_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Va2oZyjQ24v4",
   "metadata": {
    "id": "Va2oZyjQ24v4"
   },
   "source": [
    "-----\n",
    "\n",
    "# Semi-Supervised self-training CNN with `Data Augmentation`\n",
    "### Applying `Data Augmentation` on `Labeled Images`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3800a05-b45b-4672-884b-e888a62f609b",
   "metadata": {
    "id": "a3800a05-b45b-4672-884b-e888a62f609b"
   },
   "outputs": [],
   "source": [
    "# Function to generate augmented labeled data\n",
    "def augment_labeled_data(x, y, augmentation_factor):\n",
    "    \"\"\"\n",
    "    Apply data augmentation to labeled data.\n",
    "\n",
    "    Parameters:\n",
    "        x (numpy.ndarray): Labeled input images.\n",
    "        y (numpy.ndarray): Corresponding labels.\n",
    "        augmentation_factor (int): Number of augmented images to generate for each labeled image.\n",
    "\n",
    "    Returns:\n",
    "        (numpy.ndarray, numpy.ndarray): Tuple of augmented labeled images and their corresponding labels.\n",
    "    \"\"\"\n",
    "    augmented_labeled_images = []\n",
    "    augmented_labels = []\n",
    "\n",
    "    for i in range(len(x)):\n",
    "        augmented_images = augment_data(x[i:i+1], augmentation_factor)\n",
    "        augmented_labeled_images.append(augmented_images)\n",
    "        augmented_labels.extend([y[i]] * augmentation_factor)\n",
    "\n",
    "    # Reshape augmented images to match the dimensions of x\n",
    "    augmented_labeled_images = np.vstack(augmented_labeled_images).reshape(-1, 28, 28)\n",
    "\n",
    "    return augmented_labeled_images, np.array(augmented_labels)\n",
    "\n",
    "# Data Augmentation  \n",
    "augmentation_factor = 200\n",
    "augmented_x_labeled, augmented_y_labeled = augment_labeled_data(x=x_labeled, \n",
    "                                                                y=y_labeled, \n",
    "                                                                augmentation_factor=augmentation_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc7d349-7743-46f2-8c87-736395efe3fd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1fc7d349-7743-46f2-8c87-736395efe3fd",
    "outputId": "81195a20-f96c-4e19-e1a2-5f32e61e7f48"
   },
   "outputs": [],
   "source": [
    "# Create the full training data after we have done the data augmentation\n",
    "full_x_labeled = np.concatenate([x_labeled, augmented_x_labeled])\n",
    "full_y_labeled = np.concatenate([y_labeled, augmented_y_labeled])\n",
    "\n",
    "# Shuffle full_x_labeled and full_y_labeled in the same way\n",
    "shuffle_indices = np.random.permutation(len(augmented_x_labeled))\n",
    "full_x_labeled = full_x_labeled[shuffle_indices]\n",
    "full_y_labeled = full_y_labeled[shuffle_indices]\n",
    "\n",
    "# Print details about the data used for training\n",
    "print(f\"Number of original labeled samples: {len(x_labeled)}\")\n",
    "print(f\"Number of augmented samples per original labeled sample: {augmentation_factor}\")\n",
    "print(f\"Total number of labeled samples after augmentation: {len(full_x_labeled)}\")\n",
    "\n",
    "# Print details about the unlabeled data\n",
    "print(f\"Number of unlabeled samples: {len(x_unlabeled)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc003b8-b4b6-4111-b9e0-9934b9f80d9a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "9dc003b8-b4b6-4111-b9e0-9934b9f80d9a",
    "outputId": "b7289e65-78fa-4de1-e988-5e9b485ac7cd"
   },
   "outputs": [],
   "source": [
    "display_random_images(full_x_labeled, full_y_labeled, num_images=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4b8f98-6a8b-4594-b448-54f865bd7fda",
   "metadata": {
    "id": "fb4b8f98-6a8b-4594-b448-54f865bd7fda"
   },
   "source": [
    "#### `Training` and `Testing` the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da80a7cb-3b4a-4586-ae46-9f0a43f86567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the model and hyperparameters\n",
    "classifier1_augmented, classifier2_augmented = model_CNN, model_CNN\n",
    "epochs_classifier1_augmented, batch_size_classifier1_augmented= 150, 32\n",
    "epochs_classifier2_augmented, batch_size_classifier2_augmented= 100, 2000\n",
    "\n",
    "# Compile the classifier1_augmented and classifier2_augmented\n",
    "classifier1_augmented.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "classifier2_augmented.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Use EarlyStopping to avoid wasting time\n",
    "early_stop = EarlyStopping(monitor='loss', patience=3, verbose=1)\n",
    "\n",
    "# Print the number of elements used to train the first classifier\n",
    "print(f\">>>> Number of elements used to train the first classifier: {len(full_x_labeled)}\")\n",
    "\n",
    "# Train the model with the original labeled data and validation set, and save the history\n",
    "history_classifier1_augmented = classifier1_augmented.fit(\n",
    "    full_x_labeled,  # x_labeled and augmented_x_labeled\n",
    "    full_y_labeled,  # y_labeled and augmented_y_labeled\n",
    "    epochs=epochs_classifier1_augmented,\n",
    "    batch_size=batch_size_classifier1_augmented,\n",
    "    validation_data=(x_validation, y_validation),\n",
    "    callbacks=[early_stop]\n",
    ")\n",
    "\n",
    "# Pseudo-labeling\n",
    "pseudo_labeled_images, pseudo_labels, x_unlabeled_unused = pseudo_labeling(classifier1_augmented, \n",
    "                                                                           x_unlabeled,\n",
    "                                                                           confidence_threshold=0.95)\n",
    "\n",
    "if pseudo_labeled_images is not None:\n",
    "    # Update labeled data with pseudo-labeled data\n",
    "    x_labeled_and_pseudo_labeled_images = np.concatenate([full_x_labeled, pseudo_labeled_images])\n",
    "    y_labeled_and_pseudo_labeled_images = np.concatenate([full_y_labeled, pseudo_labels])\n",
    "\n",
    "    # Print the number of elements used to train the first classifier and pseudo-labeled data\n",
    "    print(\n",
    "        f\">>>> Number of elements used to train the second classifier : {len(x_labeled_and_pseudo_labeled_images) + len(x_unlabeled_unused)} ( number of augmented elements: {len(augmented_x_labeled)} )\")\n",
    "\n",
    "    # Train the classifier2 on the full dataset (original + pseudo-labeled) with validation set, and save the history\n",
    "    history_classifier2_augmented = classifier2_augmented.fit(\n",
    "        np.concatenate([x_labeled_and_pseudo_labeled_images, x_unlabeled_unused]),\n",
    "        np.concatenate([y_labeled_and_pseudo_labeled_images,\n",
    "                        np.argmax(classifier2_augmented.predict(x_unlabeled_unused), axis=1)]),\n",
    "        epochs=epochs_classifier2_augmented,\n",
    "        batch_size=batch_size_classifier2_augmented,\n",
    "        validation_data=(x_validation, y_validation),\n",
    "        callbacks=[early_stop]\n",
    "    )\n",
    "\n",
    "# Evaluate the classifier2 Performance on Test Data\n",
    "test_loss, test_acc = classifier2_augmented.evaluate(x_test, y_test)\n",
    "print(f\"classifier2 - Test Loss: {test_loss}, Test Accuracy: {test_acc}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd6ec2e-c836-41c2-a3f8-c2205ecea5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on the test set using classifier2\n",
    "y_pred_augmented = np.argmax(classifier2_augmented.predict(x_test), axis=1)\n",
    "\n",
    "# Use the function to plot and print metrics\n",
    "plot_and_print_classification_metrics(y_test, y_pred_augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae26e90a-b7df-4b6a-8605-d40635477ded",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ae26e90a-b7df-4b6a-8605-d40635477ded",
    "outputId": "3410689f-0a8d-4d3a-cc59-763fadbc6059"
   },
   "outputs": [],
   "source": [
    "# Plot the training loss and validation accuracy for each classifier with data augmentation\n",
    "plot_metrics(history_classifier1_augmented, f'Classifier 1 with Data Augmentation ( augmentation_factor = {augmentation_factor} )')\n",
    "plot_metrics(history_classifier2_augmented, f'Classifier 2 with Data Augmentation ( augmentation_factor = {augmentation_factor} )')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7003b075-549a-4696-a7cb-31c95af38b6a",
   "metadata": {
    "id": "7003b075-549a-4696-a7cb-31c95af38b6a"
   },
   "source": [
    "#### Testing the Model on an MNIST Image\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GP7HcbvMhfNO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "GP7HcbvMhfNO",
    "outputId": "7b72694b-0a69-48af-c338-c6d1225696fd"
   },
   "outputs": [],
   "source": [
    "num_samples = 5\n",
    "visualize_predictions(classifier2_augmented, x_test, y_test, num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c286bbf-ff87-4bd6-8419-2f087b360be8",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "#### Data `Augmentation` function for an image with added `Gaussian noise`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d6bd34-2783-4726-ab9c-039ffd3aba6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data_with_noise(image, augment_factor=8, noise_factor=0.1):\n",
    "    \"\"\"\n",
    "    Apply diverse data augmentation, including random noise, to a given image.\n",
    "\n",
    "    Parameters:\n",
    "        image (numpy.ndarray): Input image to be augmented.\n",
    "        augment_factor (int, optional): Number of augmented images to generate. Defaults to 8.\n",
    "        noise_factor (float, optional): Intensity of random noise to be added. Defaults to 0.1.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Array of augmented images.\n",
    "    \"\"\"\n",
    "    augmented_images = []\n",
    "    \n",
    "    # Convert the image to float32 to allow for addition with floating-point noise\n",
    "    image_float = image.astype(np.float32)\n",
    "\n",
    "    for _ in range(augment_factor):\n",
    "        # Random rotation (between -10 and 10 degrees)\n",
    "        angle = np.random.uniform(-10, 10)\n",
    "        # Random translation (between -3 and 3 pixels in both x and y directions)\n",
    "        tx = np.random.uniform(-3, 3)\n",
    "        ty = np.random.uniform(-3, 3)\n",
    "        # Random zoom (between 0.9 and 1.1)\n",
    "        zoom = np.random.uniform(0.9, 1.1)\n",
    "        # Random brightness adjustment\n",
    "        brightness_factor = np.random.uniform(0.7, 1.3)\n",
    "        # Random contrast adjustment\n",
    "        contrast_factor = np.random.uniform(0.7, 1.3)\n",
    "        \n",
    "        # Apply transformations\n",
    "        transform_matrix = cv2.getRotationMatrix2D((14, 14), angle, zoom)\n",
    "        transform_matrix[:, 2] += (tx, ty)\n",
    "        augmented_image = cv2.warpAffine(image[0], transform_matrix, (28, 28), flags=cv2.INTER_LINEAR)\n",
    "        \n",
    "        # Adjust brightness and contrast\n",
    "        augmented_image = cv2.convertScaleAbs(augmented_image, alpha=brightness_factor, beta=contrast_factor)\n",
    "        \n",
    "        # Add random noise\n",
    "        noise = np.random.normal(scale=noise_factor, size=augmented_image.shape).astype(np.float32)\n",
    "        augmented_image = image_float + noise\n",
    "        \n",
    "        # Clip the values to the valid uint8 range [0, 255]\n",
    "        augmented_image = np.clip(augmented_image, 0, 255).astype(np.uint8)\n",
    "\n",
    "        augmented_images.append(augmented_image.reshape(1, 28, 28, 1))\n",
    "\n",
    "    augmented_images = np.vstack(augmented_images)\n",
    "    return augmented_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65178314-1d19-4343-8679-77be0cbc8cd6",
   "metadata": {},
   "source": [
    "#### Testing `Data Augmentation` with `Gaussian noise` on a random MNIST image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5dce51-fec6-48db-a76f-5d2e768f0ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select one image for demonstration\n",
    "demo_index = np.random.choice(range(len(x_train_full)), size=1, replace=False)\n",
    "demo_image = x_train_full[demo_index]\n",
    "demo_label = y_train_full[demo_index]\n",
    "\n",
    "# Display original image\n",
    "visualize_images([demo_image], \"\", demo_label)\n",
    "\n",
    "# Data augmentation for demonstration\n",
    "augmented_images = augment_data_with_noise(demo_image, augment_factor=10, noise_factor=10)\n",
    "\n",
    "# Display augmented images\n",
    "augmented_labels = np.full(len(augmented_images), demo_label[0])\n",
    "visualize_images(augmented_images, \"Augmented Images\", augmented_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6911620b-743b-488f-942a-8860f7714cf9",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "# Semi-Supervised self-training with `Data Augmentation` with `Gaussian noise`\n",
    "### Applying `Data Augmentation` with `Gaussian noise` on `Labeled Images`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3efa297-06f7-4d4f-9621-7542f15a5bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate augmented labeled data with noise\n",
    "def augment_labeled_data_with_noise(x, y, augmentation_factor, noise_factor):\n",
    "    \"\"\"\n",
    "    Apply data augmentation with noise to labeled data.\n",
    "\n",
    "    Parameters:\n",
    "        x (numpy.ndarray): Labeled input images.\n",
    "        y (numpy.ndarray): Corresponding labels.\n",
    "        augmentation_factor (int): Number of augmented images to generate for each labeled image.\n",
    "        noise_factor (float): Factor to control the amount of noise to be added.\n",
    "\n",
    "    Returns:\n",
    "        (numpy.ndarray, numpy.ndarray): Tuple of augmented labeled images with noise and their corresponding labels.\n",
    "    \"\"\"\n",
    "    augmented_labeled_images = []\n",
    "    augmented_labels = []\n",
    "\n",
    "    for i in range(len(x)):\n",
    "        augmented_images = augment_data_with_noise(x[i:i+1], augmentation_factor, noise_factor)\n",
    "        augmented_labeled_images.append(augmented_images)\n",
    "        augmented_labels.extend([y[i]] * augmentation_factor)\n",
    "\n",
    "    # Reshape augmented images to match the dimensions of x\n",
    "    augmented_labeled_images = np.vstack(augmented_labeled_images).reshape(-1, 28, 28)\n",
    "\n",
    "    return augmented_labeled_images, np.array(augmented_labels)\n",
    "\n",
    "# Data Augmentation with Noise\n",
    "def augment_data_with_noise(image, augment_factor=8, noise_factor=0.1):\n",
    "    \"\"\"\n",
    "    Apply diverse data augmentation with noise to a given image.\n",
    "\n",
    "    Parameters:\n",
    "        image (numpy.ndarray): Input image to be augmented.\n",
    "        augment_factor (int): Number of augmented images to generate.\n",
    "        noise_factor (float): Factor to control the amount of noise to be added.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Array of augmented images with noise.\n",
    "    \"\"\"\n",
    "    augmented_images = []\n",
    "    for _ in range(augment_factor):\n",
    "        # Call the existing augment_data function for transformations\n",
    "        transformed_images = augment_data(image, 1)\n",
    "        # Add noise to the transformed images without clipping\n",
    "        noisy_images = add_noise(transformed_images, noise_factor)\n",
    "        augmented_images.append(noisy_images)\n",
    "\n",
    "    augmented_images = np.vstack(augmented_images)\n",
    "    return augmented_images\n",
    "\n",
    "# Function to add noise to images\n",
    "def add_noise(images, noise_factor=0.1):\n",
    "    \"\"\"\n",
    "    Add random noise to a given set of images.\n",
    "\n",
    "    Parameters:\n",
    "        images (numpy.ndarray): Input images.\n",
    "        noise_factor (float): Factor to control the amount of noise to be added.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Array of images with added noise.\n",
    "    \"\"\"\n",
    "    noisy_images = images + noise_factor * np.random.normal(0, 1, images.shape)\n",
    "    return noisy_images\n",
    "\n",
    "# Usage example\n",
    "augmentation_factor = 250\n",
    "noise_factor = 0.05\n",
    "augmented_x_labeled_with_noise, augmented_y_labeled_with_noise = augment_labeled_data_with_noise(x=x_labeled,\n",
    "                                                                            y=y_labeled,\n",
    "                                                                            augmentation_factor=augmentation_factor,\n",
    "                                                                            noise_factor=noise_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abb49be-59c7-43d0-b402-654ce676a125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the full training data after we have done the data augmentation with noise\n",
    "full_x_labeled_with_noise = np.concatenate([x_labeled, augmented_x_labeled_with_noise])\n",
    "full_y_labeled_with_noise = np.concatenate([y_labeled, augmented_y_labeled_with_noise])\n",
    "\n",
    "# Shuffle full_x_labeled_with_noise and full_y_labeled_with_noise in the same way\n",
    "shuffle_indices = np.random.permutation(len(full_x_labeled_with_noise))\n",
    "full_x_labeled_with_noise = full_x_labeled_with_noise[shuffle_indices]\n",
    "full_y_labeled_with_noise = full_y_labeled_with_noise[shuffle_indices]\n",
    "\n",
    "# Print details about the data used for training\n",
    "print(f\"Number of original labeled samples: {len(x_labeled)}\")\n",
    "print(f\"Number of augmented samples per original labeled sample: {augmentation_factor}\")\n",
    "print(f\"Total number of labeled samples after augmentation with noise: {len(full_x_labeled)}\")\n",
    "\n",
    "# Print details about the unlabeled data\n",
    "print(f\"Number of unlabeled samples: {len(x_unlabeled)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5729a5c-ca2b-443f-bce1-d5f9ffb13d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_random_images(full_x_labeled_with_noise, full_y_labeled_with_noise, num_images=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fa198a-0a6c-4967-a51b-87b57d86c31a",
   "metadata": {},
   "source": [
    "#### `Training` and `Testing` the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdf47b2-7e1f-4fe1-aa0c-bed4a668a18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the model and hyperparameters\n",
    "classifier1_augmented_with_noise, classifier2_augmented_with_noise = model_CNN, model_CNN\n",
    "epochs_classifier1_augmented_with_noise, batch_size_classifier1_augmented_with_noise= 150, 32\n",
    "epochs_classifier2_augmented_with_noise, batch_size_classifier2_augmented_with_noise= 100, 2000\n",
    "\n",
    "# Compile the classifier1_augmented and classifier2_augmented\n",
    "classifier1_augmented_with_noise.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "classifier2_augmented_with_noise.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Use EarlyStopping to avoid wasting time\n",
    "early_stop = EarlyStopping(monitor='loss', patience=3, verbose=1)\n",
    "\n",
    "# Print the number of elements used to train the first classifier\n",
    "print(f\">>>> Number of elements used to train the first classifier: {len(full_x_labeled_with_noise)}\")\n",
    "\n",
    "# Train the model with the original labeled data and validation set, and save the history\n",
    "history_classifier1_augmented_with_noise = classifier1_augmented.fit(\n",
    "    full_x_labeled_with_noise,  # x_labeled and augmented_x_labeled_with_noise\n",
    "    full_y_labeled_with_noise,  # y_labeled and augmented_y_labeled_with_noise\n",
    "    epochs=epochs_classifier1_augmented_with_noise,\n",
    "    batch_size=batch_size_classifier1_augmented_with_noise,\n",
    "    validation_data=(x_validation, y_validation),\n",
    "    callbacks=[early_stop]\n",
    ")\n",
    "\n",
    "# Pseudo-labeling\n",
    "pseudo_labeled_images, pseudo_labels, x_unlabeled_unused = pseudo_labeling(classifier1_augmented_with_noise, \n",
    "                                                                           x_unlabeled,\n",
    "                                                                           confidence_threshold=0.95)\n",
    "\n",
    "if pseudo_labeled_images is not None:\n",
    "    # Update labeled data with pseudo-labeled data\n",
    "    x_labeled_and_pseudo_labeled_images = np.concatenate([full_x_labeled, pseudo_labeled_images])\n",
    "    y_labeled_and_pseudo_labeled_images = np.concatenate([full_y_labeled, pseudo_labels])\n",
    "\n",
    "    # Print the number of elements used to train the first classifier and pseudo-labeled data\n",
    "    print(\n",
    "        f\">>>> Number of elements used to train the second classifier : {len(x_labeled_and_pseudo_labeled_images) + len(x_unlabeled_unused)} ( number of augmented elements: {len(augmented_x_labeled)} )\")\n",
    "\n",
    "    # Train the classifier2 on the full dataset (original + pseudo-labeled) with validation set, and save the history\n",
    "    history_classifier2_augmented_with_noise = classifier2_augmented_with_noise.fit(\n",
    "        np.concatenate([x_labeled_and_pseudo_labeled_images, x_unlabeled_unused]),\n",
    "        np.concatenate([y_labeled_and_pseudo_labeled_images,\n",
    "                        np.argmax(classifier2_augmented_with_noise.predict(x_unlabeled_unused), axis=1)]),\n",
    "        epochs=epochs_classifier2_augmented_with_noise,\n",
    "        batch_size=batch_size_classifier2_augmented_with_noise,\n",
    "        validation_data=(x_validation, y_validation),\n",
    "        callbacks=[early_stop]\n",
    "    )\n",
    "\n",
    "# Evaluate the classifier2 Performance on Test Data\n",
    "test_loss, test_acc = classifier2_augmented_with_noise.evaluate(x_test, y_test)\n",
    "print(f\"classifier2 - Test Loss: {test_loss}, Test Accuracy: {test_acc}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae9e0ba-6d1e-4c5d-8c24-2fe0da17ea5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on the test set using classifier2\n",
    "y_pred_augmented_with_noise = np.argmax(classifier2_augmented_with_noise.predict(x_test), axis=1)\n",
    "\n",
    "# Use the function to plot and print metrics\n",
    "plot_and_print_classification_metrics(y_test, y_pred_augmented_with_noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b72104-c713-4de4-a37c-ed66ae0357f8",
   "metadata": {},
   "source": [
    "#### Testing the Model on an MNIST Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10efad8a-bff0-495c-89c2-e6d4871802b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 5\n",
    "visualize_predictions(classifier2_augmented_with_noise, x_test, y_test, num_samples)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
